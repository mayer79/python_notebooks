{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dear ML Model, Please Multiply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how ML models can learn to approximately multiply two numbers, even when the data is noisy.\n",
    "\n",
    "### Basics\n",
    "\n",
    "A typical ML task is to estimate a function $f$ \n",
    "$$\n",
    "    y = f(x_1, x_2, \\dots) + \\varepsilon\n",
    "$$\n",
    "as good as possible by $\\hat f$ from data. Afterward, we can use $\\hat f$ to \n",
    "\n",
    "- gain insights on the relationship between $x_i$ and $y$ and\n",
    "- to make predictions based on $x_i$.\n",
    "\n",
    "The specific steps to find $\\hat f$ depend on the assumed structural form of $f$ and the optimization algorithm used to determine $\\hat f$. \n",
    "\n",
    "### Some examples\n",
    "\n",
    "- **Linear regression**: $f$ is assumed to be linear in its parameters and $\\hat f$ is found by least-squares.\n",
    "- **Generalized linear model**: $g(f)$ is linear and $\\hat f$ is found by iteratively reweighted least-squares.\n",
    "- **Neural net**: $f$ is a composition of linear and non-linear functions, found by auto-differentiation-based gradient descent.\n",
    "- **Decision tree**: $f$ is a binary decision tree, i.e. a collection of yes/no questions calculated by recursion.\n",
    "- **Random forest**: $f$ is the average of randomized decision trees.\n",
    "- **Gradient boosting**: $f$ is the average of decision trees, calculated sequentially. Each tree tries to fix the mistakes of the previous ones.\n",
    "\n",
    "### Outlook\n",
    "\n",
    "In this notebook, we will use neural nets and gradient boosting to learn noisy multiplication of two numbers $x_1$ and $x_2$, i.e.\n",
    "$$\n",
    "    y = f(x_1, x_2) + \\varepsilon = x_1 \\cdot x_2 + \\varepsilon, \n",
    "$$\n",
    "with $\\varepsilon\\sim N(0,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate noisy data\n",
    "\n",
    "Let's generate one million independent observations of above random process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x1</th>\n",
       "      <th>x2</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.427959</td>\n",
       "      <td>-0.909703</td>\n",
       "      <td>2.914672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.521623</td>\n",
       "      <td>-3.973049</td>\n",
       "      <td>-9.736590</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.307314</td>\n",
       "      <td>-1.002808</td>\n",
       "      <td>-1.534397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-6.998346</td>\n",
       "      <td>-0.479178</td>\n",
       "      <td>3.621836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-5.852353</td>\n",
       "      <td>-1.723083</td>\n",
       "      <td>10.453152</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         x1        x2          y\n",
       "0 -1.427959 -0.909703   2.914672\n",
       "1  2.521623 -3.973049  -9.736590\n",
       "2  1.307314 -1.002808  -1.534397\n",
       "3 -6.998346 -0.479178   3.621836\n",
       "4 -5.852353 -1.723083  10.453152"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Gernerate n observations\n",
    "np.random.seed(1901)\n",
    "n = 1_000_000 # 10_000_000\n",
    "df = pd.DataFrame(np.random.uniform(-10, 10, size=(n, 2)), columns=['x1', 'x2'])\n",
    "df['y'] = df.x1 * df.x2 + np.random.normal(scale=1, size=(n, ))\n",
    "df.hist(bins=100, layout=(1, 3), figsize=(15, 4))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training/Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    df.drop('y', axis=1), \n",
    "    df.y, \n",
    "    train_size=0.9, \n",
    "    random_state=525\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Net\n",
    "\n",
    "Can a neural net predict the result of the (noisy) multiplication if it fed the two inputs?\n",
    "\n",
    "We start with the most simple neural net (no hidden layers) and then, step by step, add more neurons and/or more hidden layers. \n",
    "To do so, we rely on Google's [Keras/Tensorflow](https://www.tensorflow.org/guide/keras/sequential_model?hl=en).\n",
    "\n",
    "It is up to you, dear user, to select the specific architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Plot history (dropping the first m burn-in epochs)\n",
    "def plot_history(h, drop_m=0):\n",
    "    h = pd.DataFrame(h.history.history)\n",
    "    h['epoch'] = np.arange(len(h.index)) + 1\n",
    "    h = h.iloc[drop_m:]\n",
    "    plt.plot(h.epoch, h.loss, label='Training')\n",
    "    plt.plot(h.epoch, h.val_loss, label='Validation')\n",
    "    plt.legend()\n",
    "    \n",
    "# Callbacks to stop if validation performance stops to improve and to lower the learning-rate if appropriate.\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(patience=10, mode='min')\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2)\n",
    "cb = [early_stop, reduce_lr]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_13\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_35 (Dense)             (None, 20)                60        \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 1)                 21        \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Neural net structure (play with architecture)\n",
    "def fresh_neural_net():\n",
    "    m = keras.Sequential()\n",
    "    m.add(Dense(20, activation='tanh', input_shape=(2, )))\n",
    "    m.add(Dense(1, activation='linear'))\n",
    "    m.compile(loss='mse', optimizer=keras.optimizers.Nadam(lr=0.05))\n",
    "    return m\n",
    "\n",
    "# Create new net\n",
    "neural_net = fresh_neural_net()\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic gradient descend until model does not get better anymore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 472.3255 - val_loss: 137.0303 - lr: 0.0500\n",
      "Epoch 2/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 92.8682 - val_loss: 60.4263 - lr: 0.0500\n",
      "Epoch 3/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 45.6484 - val_loss: 39.0138 - lr: 0.0500\n",
      "Epoch 4/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 30.3697 - val_loss: 28.0318 - lr: 0.0500\n",
      "Epoch 5/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 23.7744 - val_loss: 19.0956 - lr: 0.0500\n",
      "Epoch 6/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 18.3412 - val_loss: 14.1106 - lr: 0.0500\n",
      "Epoch 7/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 16.3388 - val_loss: 24.9507 - lr: 0.0500\n",
      "Epoch 8/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 14.0847 - val_loss: 10.8982 - lr: 0.0500\n",
      "Epoch 9/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 12.9459 - val_loss: 8.7799 - lr: 0.0500\n",
      "Epoch 10/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 11.2924 - val_loss: 10.2307 - lr: 0.0500\n",
      "Epoch 11/1000\n",
      "90/90 [==============================] - 0s 4ms/step - loss: 8.7956 - val_loss: 8.5919 - lr: 0.0500\n",
      "Epoch 12/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 9.6223 - val_loss: 5.9332 - lr: 0.0500\n",
      "Epoch 13/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 6.3697 - val_loss: 9.0978 - lr: 0.0500\n",
      "Epoch 14/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 9.3929 - val_loss: 7.4644 - lr: 0.0500\n",
      "Epoch 15/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 6.7256 - val_loss: 7.0339 - lr: 0.0500\n",
      "Epoch 16/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 6.9359 - val_loss: 3.7509 - lr: 0.0500\n",
      "Epoch 17/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 6.2056 - val_loss: 7.8317 - lr: 0.0500\n",
      "Epoch 18/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 7.0826 - val_loss: 4.8617 - lr: 0.0500\n",
      "Epoch 19/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.8863 - val_loss: 10.9563 - lr: 0.0500\n",
      "Epoch 20/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.6119 - val_loss: 5.9537 - lr: 0.0500\n",
      "Epoch 21/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 5.0166 - val_loss: 6.2086 - lr: 0.0500\n",
      "Epoch 22/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.9421 - val_loss: 1.8471 - lr: 0.0100\n",
      "Epoch 23/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.8363 - val_loss: 1.8169 - lr: 0.0100\n",
      "Epoch 24/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.8068 - val_loss: 1.7892 - lr: 0.0100\n",
      "Epoch 25/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.7790 - val_loss: 1.7614 - lr: 0.0100\n",
      "Epoch 26/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.7524 - val_loss: 1.7360 - lr: 0.0100\n",
      "Epoch 27/1000\n",
      "90/90 [==============================] - ETA: 0s - loss: 1.729 - 1s 7ms/step - loss: 1.7270 - val_loss: 1.7107 - lr: 0.0100\n",
      "Epoch 28/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.7028 - val_loss: 1.6873 - lr: 0.0100\n",
      "Epoch 29/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.6795 - val_loss: 1.6651 - lr: 0.0100\n",
      "Epoch 30/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.6570 - val_loss: 1.6426 - lr: 0.0100\n",
      "Epoch 31/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.6351 - val_loss: 1.6214 - lr: 0.0100\n",
      "Epoch 32/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.6140 - val_loss: 1.6008 - lr: 0.0100\n",
      "Epoch 33/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.5934 - val_loss: 1.5806 - lr: 0.0100\n",
      "Epoch 34/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.5734 - val_loss: 1.5611 - lr: 0.0100\n",
      "Epoch 35/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.5539 - val_loss: 1.5413 - lr: 0.0100\n",
      "Epoch 36/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.5348 - val_loss: 1.5231 - lr: 0.0100\n",
      "Epoch 37/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.5161 - val_loss: 1.5044 - lr: 0.0100\n",
      "Epoch 38/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.4979 - val_loss: 1.4867 - lr: 0.0100\n",
      "Epoch 39/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.4800 - val_loss: 1.4689 - lr: 0.0100\n",
      "Epoch 40/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.4627 - val_loss: 1.4516 - lr: 0.0100\n",
      "Epoch 41/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.4455 - val_loss: 1.4354 - lr: 0.0100\n",
      "Epoch 42/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.4288 - val_loss: 1.4189 - lr: 0.0100\n",
      "Epoch 43/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.4127 - val_loss: 1.4035 - lr: 0.0100\n",
      "Epoch 44/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.3965 - val_loss: 1.3872 - lr: 0.0100\n",
      "Epoch 45/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.3813 - val_loss: 1.3731 - lr: 0.0100\n",
      "Epoch 46/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.3657 - val_loss: 1.3581 - lr: 0.0100\n",
      "Epoch 47/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.3515 - val_loss: 1.3429 - lr: 0.0100\n",
      "Epoch 48/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3368 - val_loss: 1.3295 - lr: 0.0100\n",
      "Epoch 49/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.3231 - val_loss: 1.3140 - lr: 0.0100\n",
      "Epoch 50/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3097 - val_loss: 1.3016 - lr: 0.0100\n",
      "Epoch 51/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.2966 - val_loss: 1.2879 - lr: 0.0100\n",
      "Epoch 52/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.2851 - val_loss: 1.2783 - lr: 0.0100\n",
      "Epoch 53/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.2728 - val_loss: 1.2686 - lr: 0.0100\n",
      "Epoch 54/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.2620 - val_loss: 1.2705 - lr: 0.0100\n",
      "Epoch 55/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3339 - val_loss: 1.2511 - lr: 0.0100\n",
      "Epoch 56/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.3152 - val_loss: 1.3501 - lr: 0.0100\n",
      "Epoch 57/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.3698 - val_loss: 1.4422 - lr: 0.0100\n",
      "Epoch 58/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.3046 - val_loss: 1.3132 - lr: 0.0100\n",
      "Epoch 59/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.3304 - val_loss: 1.2470 - lr: 0.0100\n",
      "Epoch 60/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.3313 - val_loss: 1.2167 - lr: 0.0100\n",
      "Epoch 61/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3365 - val_loss: 1.2460 - lr: 0.0100\n",
      "Epoch 62/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.3094 - val_loss: 1.7100 - lr: 0.0100\n",
      "Epoch 63/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.2866 - val_loss: 1.3506 - lr: 0.0100\n",
      "Epoch 64/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.3249 - val_loss: 1.4166 - lr: 0.0100\n",
      "Epoch 65/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.2808 - val_loss: 1.3753 - lr: 0.0100\n",
      "Epoch 66/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1643 - val_loss: 1.1607 - lr: 0.0020\n",
      "Epoch 67/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.1592 - val_loss: 1.1589 - lr: 0.0020\n",
      "Epoch 68/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1577 - val_loss: 1.1573 - lr: 0.0020\n",
      "Epoch 69/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.1562 - val_loss: 1.1557 - lr: 0.0020\n",
      "Epoch 70/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1546 - val_loss: 1.1542 - lr: 0.0020\n",
      "Epoch 71/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1529 - val_loss: 1.1528 - lr: 0.0020\n",
      "Epoch 72/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1513 - val_loss: 1.1509 - lr: 0.0020\n",
      "Epoch 73/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.1495 - val_loss: 1.1490 - lr: 0.0020\n",
      "Epoch 74/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1478 - val_loss: 1.1474 - lr: 0.0020\n",
      "Epoch 75/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1461 - val_loss: 1.1458 - lr: 0.0020\n",
      "Epoch 76/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1442 - val_loss: 1.1437 - lr: 0.0020\n",
      "Epoch 77/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1423 - val_loss: 1.1420 - lr: 0.0020\n",
      "Epoch 78/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.1404 - val_loss: 1.1409 - lr: 0.0020\n",
      "Epoch 79/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1386 - val_loss: 1.1375 - lr: 0.0020\n",
      "Epoch 80/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1365 - val_loss: 1.1359 - lr: 0.0020\n",
      "Epoch 81/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1344 - val_loss: 1.1338 - lr: 0.0020\n",
      "Epoch 82/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1324 - val_loss: 1.1326 - lr: 0.0020\n",
      "Epoch 83/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1303 - val_loss: 1.1303 - lr: 0.0020\n",
      "Epoch 84/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.1282 - val_loss: 1.1284 - lr: 0.0020\n",
      "Epoch 85/1000\n",
      "90/90 [==============================] - 1s 8ms/step - loss: 1.1261 - val_loss: 1.1256 - lr: 0.0020\n",
      "Epoch 86/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.1239 - val_loss: 1.1240 - lr: 0.0020\n",
      "Epoch 87/1000\n",
      "90/90 [==============================] - 1s 11ms/step - loss: 1.1216 - val_loss: 1.1211 - lr: 0.0020\n",
      "Epoch 88/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.1193 - val_loss: 1.1191 - lr: 0.0020\n",
      "Epoch 89/1000\n",
      "90/90 [==============================] - 1s 7ms/step - loss: 1.1173 - val_loss: 1.1166 - lr: 0.0020\n",
      "Epoch 90/1000\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 1.1150 - val_loss: 1.1142 - lr: 0.0020\n",
      "Epoch 91/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1128 - val_loss: 1.1118 - lr: 0.0020\n",
      "Epoch 92/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1106 - val_loss: 1.1094 - lr: 0.0020\n",
      "Epoch 93/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.1082 - val_loss: 1.1072 - lr: 0.0020\n",
      "Epoch 94/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1061 - val_loss: 1.1049 - lr: 0.0020\n",
      "Epoch 95/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.1038 - val_loss: 1.1039 - lr: 0.0020\n",
      "Epoch 96/1000\n",
      "90/90 [==============================] - 0s 6ms/step - loss: 1.1016 - val_loss: 1.1006 - lr: 0.0020\n",
      "Epoch 97/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0994 - val_loss: 1.0994 - lr: 0.0020\n",
      "Epoch 98/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0971 - val_loss: 1.0984 - lr: 0.0020\n",
      "Epoch 99/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0947 - val_loss: 1.0946 - lr: 0.0020\n",
      "Epoch 100/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0928 - val_loss: 1.0933 - lr: 0.0020\n",
      "Epoch 101/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0908 - val_loss: 1.0896 - lr: 0.0020\n",
      "Epoch 102/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0886 - val_loss: 1.0876 - lr: 0.0020\n",
      "Epoch 103/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0873 - val_loss: 1.0867 - lr: 0.0020\n",
      "Epoch 104/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0846 - val_loss: 1.0829 - lr: 0.0020\n",
      "Epoch 105/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0828 - val_loss: 1.0822 - lr: 0.0020\n",
      "Epoch 106/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0813 - val_loss: 1.0796 - lr: 0.0020\n",
      "Epoch 107/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0789 - val_loss: 1.0774 - lr: 0.0020\n",
      "Epoch 108/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0772 - val_loss: 1.0815 - lr: 0.0020\n",
      "Epoch 109/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0756 - val_loss: 1.0745 - lr: 0.0020\n",
      "Epoch 110/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0734 - val_loss: 1.0716 - lr: 0.0020\n",
      "Epoch 111/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0727 - val_loss: 1.0762 - lr: 0.0020\n",
      "Epoch 112/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0708 - val_loss: 1.0717 - lr: 0.0020\n",
      "Epoch 113/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0693 - val_loss: 1.0692 - lr: 0.0020\n",
      "Epoch 114/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0667 - val_loss: 1.0651 - lr: 0.0020\n",
      "Epoch 115/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0659 - val_loss: 1.0671 - lr: 0.0020\n",
      "Epoch 116/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0642 - val_loss: 1.0641 - lr: 0.0020\n",
      "Epoch 117/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0631 - val_loss: 1.0676 - lr: 0.0020\n",
      "Epoch 118/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0634 - val_loss: 1.0590 - lr: 0.0020\n",
      "Epoch 119/1000\n",
      "90/90 [==============================] - 0s 5ms/step - loss: 1.0593 - val_loss: 1.0616 - lr: 0.0020\n",
      "Epoch 120/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0588 - val_loss: 1.0571 - lr: 0.0020\n",
      "Epoch 121/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0589 - val_loss: 1.0564 - lr: 0.0020\n",
      "Epoch 122/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0558 - val_loss: 1.0573 - lr: 0.0020\n",
      "Epoch 123/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.0554 - val_loss: 1.0572 - lr: 0.0020\n",
      "Epoch 124/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0542 - val_loss: 1.0534 - lr: 0.0020\n",
      "Epoch 125/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0535 - val_loss: 1.0521 - lr: 0.0020\n",
      "Epoch 126/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0527 - val_loss: 1.0514 - lr: 0.0020\n",
      "Epoch 127/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0511 - val_loss: 1.0483 - lr: 0.0020\n",
      "Epoch 128/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.0496 - val_loss: 1.0483 - lr: 0.0020\n",
      "Epoch 129/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0484 - val_loss: 1.0499 - lr: 0.0020\n",
      "Epoch 130/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0476 - val_loss: 1.0494 - lr: 0.0020\n",
      "Epoch 131/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0470 - val_loss: 1.0471 - lr: 0.0020\n",
      "Epoch 132/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0460 - val_loss: 1.0450 - lr: 0.0020\n",
      "Epoch 133/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0453 - val_loss: 1.0478 - lr: 0.0020\n",
      "Epoch 134/1000\n",
      "90/90 [==============================] - 1s 6ms/step - loss: 1.0443 - val_loss: 1.0444 - lr: 0.0020\n",
      "Epoch 135/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.0447 - val_loss: 1.0465 - lr: 0.0020\n",
      "Epoch 136/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.0428 - val_loss: 1.0435 - lr: 0.0020\n",
      "Epoch 137/1000\n",
      "90/90 [==============================] - 1s 10ms/step - loss: 1.0431 - val_loss: 1.0412 - lr: 0.0020\n",
      "Epoch 138/1000\n",
      "90/90 [==============================] - 1s 9ms/step - loss: 1.0414 - val_loss: 1.0392 - lr: 0.0020\n",
      "Epoch 139/1000\n",
      "22/90 [======>.......................] - ETA: 0s - loss: 1.0360"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-0d01be2deef2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m neural_net.fit(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    783\u001b[0m                 batch_size=batch_size):\n\u001b[0;32m    784\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 785\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    786\u001b[0m               \u001b[1;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    787\u001b[0m               \u001b[1;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    579\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 580\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    581\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    582\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    610\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 611\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    612\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    613\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2418\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2420\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2421\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2422\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1660\u001b[0m     \"\"\"\n\u001b[1;32m-> 1661\u001b[1;33m     return self._call_flat(\n\u001b[0m\u001b[0;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1744\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1745\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    591\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "neural_net.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    batch_size=10000, \n",
    "    epochs=1000, \n",
    "    validation_data = (X_valid, y_valid),\n",
    "    callbacks=cb,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_history(neural_net, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23.990229]], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net.predict([[-3, -8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation with numbers outside [-10, 10]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.78042775]], dtype=float32)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_net.predict([[4, 40]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model works, but only within the range of samples available. Extrapolation is not possible without manual feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural nets are especially successful with text, image, and sound data. For tabular data, other methods like gradient boosting are often better and easier to use. Hoever, since the normal person has never heard of GLMs, random forests or gradient boosting, it is easier to mention neural nets if someone asks about what we do...\n",
    "\n",
    "Here, we fit a [LightGBM model](https://lightgbm.readthedocs.io/en/latest), a gradient boosting algorithm implementation by Microsoft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds\n",
      "[100]\tvalid_0's l2: 1.23496\n",
      "[200]\tvalid_0's l2: 1.12553\n",
      "[300]\tvalid_0's l2: 1.09257\n",
      "[400]\tvalid_0's l2: 1.07608\n",
      "[500]\tvalid_0's l2: 1.06854\n",
      "[600]\tvalid_0's l2: 1.06417\n",
      "[700]\tvalid_0's l2: 1.06178\n",
      "[800]\tvalid_0's l2: 1.06043\n",
      "Early stopping, best iteration is:\n",
      "[876]\tvalid_0's l2: 1.05976\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "# Internal data handler\n",
    "dtrain = lgb.Dataset(X_train, y_train)\n",
    "dvalid = lgb.Dataset(X_valid, y_valid)\n",
    "\n",
    "# Parameters\n",
    "params = {\n",
    "    'learning_rate': 0.2,\n",
    "    'num_leaves': 63,\n",
    "    'min_child_samples': 10,\n",
    "    'objective': 'mse'\n",
    "}\n",
    "\n",
    "# Fit until validation performance starts to deteriorate\n",
    "fit = lgb.train(\n",
    "    params, \n",
    "    train_set=dtrain,\n",
    "    early_stopping_rounds=20,\n",
    "    num_boost_round=10000,\n",
    "    valid_sets=dvalid,\n",
    "    verbose_eval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([24.15381159])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.predict([[-3, -8]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation with numbers outside [-10, 10]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([29.82848241])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit.predict([[20, 3]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is reality trickier?\n",
    "- More than two input variables\n",
    "- Missing values\n",
    "- Outliers or data errors\n",
    "- Special data structure (clusters, time series, spatial, ...)\n",
    "- Extrapolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
