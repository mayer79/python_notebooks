{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How does a neural net work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Illustrated for the task to learn to multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "We generate many rows from the true \"model\"\n",
    "$$\n",
    " y = f(x_1, x_2) = x_1 x_2,\n",
    "$$\n",
    "and want to estimate $f$ by $\\hat f$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate n observations\n",
    "np.random.seed(1901)\n",
    "n = 1_000_000 # 10_000_000\n",
    "\n",
    "df = pd.DataFrame(np.random.uniform(-10, 10, size=(n, 2)), \n",
    "                  columns=['x1', 'x2'])\n",
    "df['y'] = df.x1 * df.x2 #+ np.random.normal(scale=1, size=(n, ))\n",
    "\n",
    "# Visualize\n",
    "df.hist(bins=100, layout=(1, 3), figsize=(15, 4))\n",
    "df.head()\n",
    "\n",
    "# Save first few rows\n",
    "# df[0:1000].to_excel(\"Data.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.regplot(x='x1', y='y', data=df, ci=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Linear regression as neuronal net\n",
    "\n",
    "We start with a simple neural net to mimic a linear regression with two covariables. Do not expect too much as one cannot represent a multiplication by a linear function (= weighted sum)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Nadam, Adam\n",
    "\n",
    "# Plot history (dropping the first m burn-in epochs)\n",
    "def plot_history(h, drop_m=0):\n",
    "    h = pd.DataFrame(h.history.history)\n",
    "    h['epoch'] = np.arange(len(h.index)) + 1\n",
    "    h = h.iloc[drop_m:]\n",
    "    plt.plot(h.epoch, h.loss, label='Training')\n",
    "    plt.plot(h.epoch, h.val_loss, label='Validation')\n",
    "    plt.legend()\n",
    "    \n",
    "# Callbacks\n",
    "cb = [EarlyStopping(patience=10),\n",
    "      ReduceLROnPlateau(patience=3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "inputs = Input(shape=(2, ))\n",
    "output = Dense(1)(inputs)\n",
    "neural_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "neural_net.compile(loss='mse', \n",
    "                   optimizer=Nadam(lr=0.2))\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.fit(\n",
    "    df[['x1', 'x2']],\n",
    "    df['y'],\n",
    "    batch_size=10000, \n",
    "    epochs=100, \n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimated parameter\n",
    "\n",
    "Since a multiplication cannot be written as weighted sum, the parameters are essentially 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.get_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compared with linear least squares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "model = sm.OLS(df['y'], sm.add_constant(df[['x1', 'x2']]))\n",
    "results = model.fit()\n",
    "results.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 2\n",
    "pred = neural_net.predict([[x1, x2]]).flatten()[0]\n",
    "print(f'{x1} times {x2} is {pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this step, we introduce hidden layers. Their additional parameters will give the neural network the ability to learn more complex relationships between in- and output like e.g. interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "inputs = Input(shape=(2, ))\n",
    "hidden = Dense(5)(inputs)\n",
    "output = Dense(1)(hidden)\n",
    "neural_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "neural_net.compile(loss='mse', \n",
    "                   optimizer=Nadam(lr=0.2),\n",
    "                   metrics=['mse'])\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.fit(\n",
    "    df[['x1', 'x2']],\n",
    "    df['y'],\n",
    "    batch_size=10000, \n",
    "    epochs=1000, \n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 2\n",
    "x2 = 2\n",
    "pred = neural_net.predict([[x1, x2]]).flatten()[0]\n",
    "print(f'{x1} times {x2} is {pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmmm..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Activation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear function of linear functions remains a linear function. We need to introduce some form of non-linearity. This is done by transforming the values of the nodes on the hidden layers by a non-linear function called \"activation function\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structur\n",
    "inputs = Input(shape=(2, ))\n",
    "hidden = Dense(5, activation='tanh')(inputs)\n",
    "output = Dense(1)(hidden)\n",
    "neural_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "neural_net.compile(loss='mse', \n",
    "                   optimizer=Nadam(lr=0.1),\n",
    "                   metrics=['mse'])\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.fit(\n",
    "    df[['x1', 'x2']],\n",
    "    df['y'],\n",
    "    batch_size=5000, \n",
    "    epochs=400, \n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(neural_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 5\n",
    "x2 = 5\n",
    "pred = neural_net.predict([[x1, x2]]).flatten()[0]\n",
    "print(f'{x1} times {x2} is {pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extrapolation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = 50\n",
    "x2 = 2\n",
    "pred = neural_net.predict([[x1, x2]]).flatten()[0]\n",
    "print(f'{x1} times {x2} is {pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4? Play!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structure\n",
    "inputs = Input(shape=(2, ))\n",
    "hidden1 = Dense(50, activation='tanh')(inputs)\n",
    "hidden2 = Dense(10, activation='tanh')(hidden1)\n",
    "output = Dense(1)(hidden2)\n",
    "neural_net = Model(inputs=inputs, outputs=output)\n",
    "\n",
    "neural_net.compile(loss='mse', optimizer=Nadam(lr=0.002))\n",
    "neural_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net.fit(\n",
    "    df[['x1', 'x2']],\n",
    "    df['y'],\n",
    "    batch_size=5000, \n",
    "    epochs=200, \n",
    "    validation_split=0.2,\n",
    "    callbacks=cb,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = -7\n",
    "x2 = 4\n",
    "pred = neural_net.predict([[x1, x2]]).flatten()[0]\n",
    "print(f'{x1} times {x2} is {pred:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multiply",
   "language": "python",
   "name": "multiply"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
